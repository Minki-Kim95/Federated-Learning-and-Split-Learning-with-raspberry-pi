{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10 Federated Resnet20 Server Side\n",
    "This code is the server part of CIFAR10 federated resnet20 for **multi** client and a server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds = 100\n",
    "local_epoch = 1\n",
    "users = 1 # number of clients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "\n",
    "import socket\n",
    "import struct\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from threading import Thread\n",
    "from threading import Lock\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn.init as init\n",
    "\n",
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet20():\n",
    "    return ResNet(BasicBlock, [3, 3, 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNet(\n",
       "  (classifer): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  )\n",
       "  (feature): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (11): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (12): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (13): Sequential(\n",
       "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_net = resnet20()\n",
    "res_net.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "clientsoclist = [0]*users\n",
    "\n",
    "start_time = 0\n",
    "weight_count = 0\n",
    "\n",
    "global_weights = copy.deepcopy(res_net.state_dict())\n",
    "\n",
    "datasetsize = [0]*users\n",
    "weights_list = [0]*users\n",
    "\n",
    "lock = Lock()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comunication overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sendsize_list = []\n",
    "total_receivesize_list = []\n",
    "\n",
    "client_sendsize_list = [[] for i in range(users)]\n",
    "client_receivesize_list = [[] for i in range(users)]\n",
    "\n",
    "train_sendsize_list = [] \n",
    "train_receivesize_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Socket initialization\n",
    "### Set host address and port number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required socket functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_msg(sock, msg):\n",
    "    # prefix each message with a 4-byte length in network byte order\n",
    "    msg = pickle.dumps(msg)\n",
    "    l_send = len(msg)\n",
    "    msg = struct.pack('>I', l_send) + msg\n",
    "    sock.sendall(msg)\n",
    "    return l_send\n",
    "\n",
    "def recv_msg(sock):\n",
    "    # read message length and unpack it into an integer\n",
    "    raw_msglen = recvall(sock, 4)\n",
    "    if not raw_msglen:\n",
    "        return None\n",
    "    msglen = struct.unpack('>I', raw_msglen)[0]\n",
    "    # read the message data\n",
    "    msg =  recvall(sock, msglen)\n",
    "    msg = pickle.loads(msg)\n",
    "    return msg, msglen\n",
    "\n",
    "def recvall(sock, n):\n",
    "    # helper function to receive n bytes or return None if EOF is hit\n",
    "    data = b''\n",
    "    while len(data) < n:\n",
    "        packet = sock.recv(n - len(data))\n",
    "        if not packet:\n",
    "            return None\n",
    "        data += packet\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def average_weights(w, datasize):\n",
    "    \"\"\"\n",
    "    Returns the average of the weights.\n",
    "    \"\"\"\n",
    "        \n",
    "    for i, data in enumerate(datasize):\n",
    "        for key in w[i].keys():\n",
    "            w[i][key] *= float(data)\n",
    "    \n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    \n",
    "    \n",
    "\n",
    "# when client use only one kinds of device\n",
    "\n",
    "    for key in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[key] += w[i][key]\n",
    "        w_avg[key] = torch.div(w_avg[key], float(sum(datasize)))\n",
    "\n",
    "# when client use various devices (cpu, gpu) you need to use it instead\n",
    "#\n",
    "#     for key, val in w_avg.items():\n",
    "#         common_device = val.device\n",
    "#         break\n",
    "#     for key in w_avg.keys():\n",
    "#         for i in range(1, len(w)):\n",
    "#             if common_device == 'cpu':\n",
    "#                 w_avg[key] += w[i][key].cpu()\n",
    "#             else:\n",
    "#                 w_avg[key] += w[i][key].cuda()\n",
    "#         w_avg[key] = torch.div(w_avg[key], float(sum(datasize)))\n",
    "\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thread define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receive users before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_thread(func, num_user):\n",
    "    global clientsoclist\n",
    "    global start_time\n",
    "    \n",
    "    thrs = []\n",
    "    for i in range(num_user):\n",
    "        conn, addr = s.accept()\n",
    "        print('Conntected with', addr)\n",
    "        # append client socket on list\n",
    "        clientsoclist[i] = conn\n",
    "        args = (i, num_user, conn)\n",
    "        thread = Thread(target=func, args=args)\n",
    "        thrs.append(thread)\n",
    "        thread.start()\n",
    "    print(\"timmer start!\")\n",
    "    start_time = time.time()    # store start time\n",
    "    for thread in thrs:\n",
    "        thread.join()\n",
    "    end_time = time.time()  # store end time\n",
    "    print(\"TrainingTime: {} sec\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def receive(userid, num_users, conn): #thread for receive clients\n",
    "    global weight_count\n",
    "    \n",
    "    global datasetsize\n",
    "\n",
    "\n",
    "    msg = {\n",
    "        'rounds': rounds,\n",
    "        'client_id': userid,\n",
    "        'local_epoch': local_epoch\n",
    "    }\n",
    "\n",
    "    datasize = send_msg(conn, msg)    #send epoch\n",
    "    total_sendsize_list.append(datasize)\n",
    "    client_sendsize_list[userid].append(datasize)\n",
    "\n",
    "    train_dataset_size, datasize = recv_msg(conn)    # get total_batch of train dataset\n",
    "    total_receivesize_list.append(datasize)\n",
    "    client_receivesize_list[userid].append(datasize)\n",
    "    \n",
    "    \n",
    "    with lock:\n",
    "        datasetsize[userid] = train_dataset_size\n",
    "        weight_count += 1\n",
    "    \n",
    "    train(userid, train_dataset_size, num_users, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(userid, train_dataset_size, num_users, client_conn):\n",
    "    global weights_list\n",
    "    global global_weights\n",
    "    global weight_count\n",
    "    global res_net\n",
    "    global val_acc\n",
    "    \n",
    "    for r in range(rounds):\n",
    "        with lock:\n",
    "            if weight_count == num_users:\n",
    "                for i, conn in enumerate(clientsoclist):\n",
    "                    datasize = send_msg(conn, global_weights)\n",
    "                    total_sendsize_list.append(datasize)\n",
    "                    client_sendsize_list[i].append(datasize)\n",
    "                    train_sendsize_list.append(datasize)\n",
    "                    weight_count = 0\n",
    "\n",
    "        client_weights, datasize = recv_msg(client_conn)\n",
    "        total_receivesize_list.append(datasize)\n",
    "        client_receivesize_list[userid].append(datasize)\n",
    "        train_receivesize_list.append(datasize)\n",
    "\n",
    "        weights_list[userid] = client_weights\n",
    "        print(\"User\" + str(userid) + \"'s Round \" + str(r + 1) +  \" is done\")\n",
    "        with lock:\n",
    "            weight_count += 1\n",
    "            if weight_count == num_users:\n",
    "                #average\n",
    "                global_weights = average_weights(weights_list, datasetsize)\n",
    "                \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192.168.83.1\n"
     ]
    }
   ],
   "source": [
    "host = socket.gethostbyname(socket.gethostname())\n",
    "port = 10080\n",
    "print(host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = socket.socket()\n",
    "s.bind((host, port))\n",
    "s.listen(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the server socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conntected with ('192.168.83.1', 5808)\n",
      "timmer start!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rlaal\\anaconda3\\envs\\py36\\lib\\site-packages\\torch\\storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User0's Round 1 is done\n",
      "TrainingTime: 12.060287475585938 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rlaal\\anaconda3\\envs\\py36\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\rlaal\\anaconda3\\envs\\py36\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-11-86c8b857c02f>\", line 26, in receive\n",
      "    train(userid, train_dataset_size, num_users, conn)\n",
      "  File \"<ipython-input-12-98f4f091f7fc>\", line 29, in train\n",
      "    global_weights = average_weights(weights_list, datasetsize)\n",
      "  File \"<ipython-input-9-c28abdcbbc9a>\", line 10, in average_weights\n",
      "    w[i][key] *= float(data)\n",
      "RuntimeError: result type Float can't be cast to the desired output type Long\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_thread(receive, users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingTime: 12.06924557685852 sec\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()  # store end time\n",
    "print(\"TrainingTime: {} sec\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print all of communication overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---total_sendsize_list---\n",
      "total_sendsize size: 13069937 bytes\n",
      "number of total_send:  2\n",
      "\n",
      "\n",
      "---total_receivesize_list---\n",
      "total receive sizes: 13069881 bytes\n",
      "number of total receive:  2\n",
      "\n",
      "\n",
      "---client_sendsize_list(user0)---\n",
      "total client_sendsizes(user0): 13069937 bytes\n",
      "number of client_send(user0):  2\n",
      "\n",
      "\n",
      "---client_receivesize_list(user0)---\n",
      "total client_receive sizes(user0): 13069881 bytes\n",
      "number of client_send(user0):  2\n",
      "\n",
      "\n",
      "---train_sendsize_list---\n",
      "total train_sendsizes: 13069876 bytes\n",
      "number of train_send:  1\n",
      "\n",
      "\n",
      "---train_receivesize_list---\n",
      "total train_receivesizes: 13069876 bytes\n",
      "number of train_receive:  1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# def commmunication_overhead():  \n",
    "print('\\n')\n",
    "print('---total_sendsize_list---')\n",
    "total_size = 0\n",
    "for size in total_sendsize_list:\n",
    "#     print(size)\n",
    "    total_size += size\n",
    "print(\"total_sendsize size: {} bytes\".format(total_size))\n",
    "print(\"number of total_send: \", len(total_sendsize_list))\n",
    "print('\\n')\n",
    "\n",
    "print('---total_receivesize_list---')\n",
    "total_size = 0\n",
    "for size in total_receivesize_list:\n",
    "#     print(size)\n",
    "    total_size += size\n",
    "print(\"total receive sizes: {} bytes\".format(total_size) )\n",
    "print(\"number of total receive: \", len(total_receivesize_list) )\n",
    "print('\\n')\n",
    "\n",
    "for i in range(users):\n",
    "    print('---client_sendsize_list(user{})---'.format(i))\n",
    "    total_size = 0\n",
    "    for size in client_sendsize_list[i]:\n",
    "#         print(size)\n",
    "        total_size += size\n",
    "    print(\"total client_sendsizes(user{}): {} bytes\".format(i, total_size))\n",
    "    print(\"number of client_send(user{}): \".format(i), len(client_sendsize_list[i]))\n",
    "    print('\\n')\n",
    "\n",
    "    print('---client_receivesize_list(user{})---'.format(i))\n",
    "    total_size = 0\n",
    "    for size in client_receivesize_list[i]:\n",
    "#         print(size)\n",
    "        total_size += size\n",
    "    print(\"total client_receive sizes(user{}): {} bytes\".format(i, total_size))\n",
    "    print(\"number of client_send(user{}): \".format(i), len(client_receivesize_list[i]))\n",
    "    print('\\n')\n",
    "\n",
    "print('---train_sendsize_list---')\n",
    "total_size = 0\n",
    "for size in train_sendsize_list:\n",
    "#     print(size)\n",
    "    total_size += size\n",
    "print(\"total train_sendsizes: {} bytes\".format(total_size))\n",
    "print(\"number of train_send: \", len(train_sendsize_list) )\n",
    "print('\\n')\n",
    "\n",
    "print('---train_receivesize_list---')\n",
    "total_size = 0\n",
    "for size in train_receivesize_list:\n",
    "#     print(size)\n",
    "    total_size += size\n",
    "print(\"total train_receivesizes: {} bytes\".format(total_size))\n",
    "print(\"number of train_receive: \", len(train_receivesize_list) )\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '../../models/cifar10_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10 (root=root_path, train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10 (root=root_path, train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataLoader` for batch generating\n",
    "`torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of total batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "2500\n"
     ]
    }
   ],
   "source": [
    "train_total_batch = len(trainloader)\n",
    "print(train_total_batch)\n",
    "test_batch = len(testloader)\n",
    "print(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_net.load_state_dict(global_weights)\n",
    "res_net.eval()\n",
    "res_net = res_net.to(device)\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = optim.SGD(res_net.parameters(), lr=lr, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of train and each of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc: 10.00%, train_loss: 2.3031\n",
      "test_acc: 10.00%, test_loss: 2.3031\n",
      "Accuracy of plane :  0 %\n",
      "Accuracy of   car :  0 %\n",
      "Accuracy of  bird :  0 %\n",
      "Accuracy of   cat : 100 %\n",
      "Accuracy of  deer :  0 %\n",
      "Accuracy of   dog :  0 %\n",
      "Accuracy of  frog :  0 %\n",
      "Accuracy of horse :  0 %\n",
      "Accuracy of  ship :  0 %\n",
      "Accuracy of truck :  0 %\n",
      "WorkingTime: 737.2920582294464 sec\n"
     ]
    }
   ],
   "source": [
    "# train acc\n",
    "with torch.no_grad():\n",
    "    corr_num = 0\n",
    "    total_num = 0\n",
    "    train_loss = 0.0\n",
    "    for j, trn in enumerate(trainloader):\n",
    "        trn_x, trn_label = trn\n",
    "        trn_x = trn_x.to(device)\n",
    "        trn_label = trn_label.clone().detach().long().to(device)\n",
    "\n",
    "        trn_output = res_net(trn_x)\n",
    "        loss = criterion(trn_output, trn_label)\n",
    "        train_loss += loss.item()\n",
    "        model_label = trn_output.argmax(dim=1)\n",
    "        corr = trn_label[trn_label == model_label].size(0)\n",
    "        corr_num += corr\n",
    "        total_num += trn_label.size(0)\n",
    "    print(\"train_acc: {:.2f}%, train_loss: {:.4f}\".format(corr_num / total_num * 100, train_loss / len(trainloader)))\n",
    "\n",
    "\n",
    "# test acc\n",
    "with torch.no_grad():\n",
    "    corr_num = 0\n",
    "    total_num = 0\n",
    "    val_loss = 0.0\n",
    "    for j, val in enumerate(testloader):\n",
    "        val_x, val_label = val\n",
    "        val_x = val_x.to(device)\n",
    "        val_label = val_label.clone().detach().long().to(device)\n",
    "\n",
    "        val_output = res_net(val_x)\n",
    "        loss = criterion(val_output, val_label)\n",
    "        val_loss += loss.item()\n",
    "        model_label = val_output.argmax(dim=1)\n",
    "        corr = val_label[val_label == model_label].size(0)\n",
    "        corr_num += corr\n",
    "        total_num += val_label.size(0)\n",
    "        accuracy = corr_num / total_num * 100\n",
    "        test_loss = val_loss / len(testloader)\n",
    "    print(\"test_acc: {:.2f}%, test_loss: {:.4f}\".format( accuracy, test_loss))\n",
    "\n",
    "# acc of each acc    \n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        x, labels = data\n",
    "        x = x.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = eres_netcg_net(x)\n",
    "        labels = labels.long()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "\n",
    "# Let's quickly save our trained model:\n",
    "PATH = './cifar10_fd_resnet20.pth'\n",
    "torch.save(res_net.state_dict(), PATH)\n",
    "\n",
    "end_time = time.time()  # store end time\n",
    "print(\"WorkingTime: {} sec\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
